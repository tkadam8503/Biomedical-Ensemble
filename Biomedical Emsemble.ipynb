{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5aaf9501-ead8-49b5-8b4a-99780a595f3f",
   "metadata": {},
   "source": [
    "# 🧬 Robust Biomedical Image Classification with a Lightweight Ensemble\n",
    "\n",
    "This project tackles medical image recognition under **practical constraints**: limited compute, unstable networks, and Windows quirks. We design a **failsafe training notebook** that prefers OCTMNIST but will gracefully fall back to alternatives so that **the demo always runs**. The final system blends three models into a single **ensemble** that improves robustness over any individual network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2157772d-59a8-4528-ae99-46cd34c45da7",
   "metadata": {},
   "source": [
    "## 🎯 Problem Statement\n",
    "\n",
    "Medical imaging pipelines often need to run in constrained environments where datasets or pretrained models may be unavailable. Our goal is to:\n",
    "- Train a **reliable classifier** for grayscale images (OCT retina slices, or fallbacks),\n",
    "- Keep the code **small, readable, and Windows-safe**, and\n",
    "- Produce **stable results** via an **ensemble** that reduces variance across models.\n",
    "\n",
    "### Key Objectives\n",
    "1. **Portability:** No brittle dependencies; works on CPU/Windows.\n",
    "2. **Fallbacks:** OCTMNIST → FashionMNIST → Synthetic, selected automatically.\n",
    "3. **Simplicity:** Minimal code, small cells, clear explanations.\n",
    "4. **Robustness:** Confidence-weighted ensemble for stronger predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2a2d3d-1c52-46bb-acee-cd86378533a2",
   "metadata": {},
   "source": [
    "## 🧪 Methodology\n",
    "\n",
    "1. **Data Loading (with Fallbacks):** Attempt OCTMNIST → FashionMNIST → Synthetic.  \n",
    "2. **Transforms (Windows-safe):** No `lambda`; use a top-level `AddGaussianNoise` class to avoid pickling issues.  \n",
    "3. **Models:**  \n",
    "   - **ResNet18** adapted for 1-channel input  \n",
    "   - **DenseNet121** patched for 28×28 (stride=1, no initial pooling)  \n",
    "   - **SimpleCNN** lightweight baseline  \n",
    "4. **Training:** Cross-entropy loss, Adam optimizer, small number of epochs for quick runs.  \n",
    "5. **Evaluation:** Validation accuracy and loss per model.  \n",
    "6. **Ensemble:** Confidence-weighted average of softmax outputs.  \n",
    "7. **Artifacts:** Save checkpoints to `./checkpoints_oct`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afeb1d8a-c462-4bd9-bc87-796114818806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install -q torch torchvision medmnist tqdm scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95105009-f660-4e77-bfab-cecd70064585",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, math, time, json, pathlib, numpy as np\n",
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Subset, TensorDataset\n",
    "import torchvision.transforms as T\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, Dict\n",
    "from torchvision.datasets import FashionMNIST\n",
    "import medmnist\n",
    "from medmnist import OCTMNIST\n",
    "import torch\n",
    "from torchvision.models import densenet121"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee11a8a6-12c6-44bc-95fb-c2676b698651",
   "metadata": {},
   "source": [
    "## ⚙️ Training Setup\n",
    "\n",
    "- **Optimizer:** Adam (lr=1e-3)\n",
    "- **Loss:** Cross-Entropy\n",
    "- **Batch Sizes:** 64 (train), 128 (val)\n",
    "- **Epochs:** 1 by default (fast demo); scale up for better accuracy\n",
    "- **Subsets:** `TRAIN_SUBSET=2000`, `VAL_SUBSET=500` to keep runs quick in demos\n",
    "\n",
    "### Why these choices?\n",
    "The emphasis is on **reproducibility and portability** over raw leaderboard performance. You can dial up epochs and remove subset limits when running on more capable hardware.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c619d27-de9f-494d-b58c-229a1b0f8693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "PRINT_EVERY = 50\n",
    "\n",
    "# training config\n",
    "BATCH_SIZE = 64\n",
    "VAL_BATCH = 128\n",
    "EPOCHS = 1               # increase for better accuracy\n",
    "TRAIN_SUBSET = 2000      # set None to use full training set\n",
    "VAL_SUBSET = 500\n",
    "\n",
    "CHECKPOINT_DIR = \"./checkpoints_oct\"\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "print(\"Device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2220f5-223a-40f3-9905-ed1ae7548c59",
   "metadata": {},
   "source": [
    "## 🗂️ Datasets\n",
    "\n",
    "### OCTMNIST (preferred)\n",
    "- **Domain:** Optical Coherence Tomography (OCT) retina images.\n",
    "- **Format:** Grayscale, 28×28 (MedMNIST version).\n",
    "- **Task:** Multi-class disease classification.\n",
    "- **Why here:** Realistic clinical modality; lightweight benchmark.\n",
    "\n",
    "### FashionMNIST (fallback)\n",
    "- **Domain:** Clothing item images.\n",
    "- **Purpose:** Stand-in when medical data cannot be downloaded; same grayscale 28×28 format.\n",
    "\n",
    "### Synthetic Blobs (last resort)\n",
    "- **Domain:** Generated shapes with noise.\n",
    "- **Purpose:** Guarantees the full training/evaluation loop runs even offline.\n",
    "- **Benefit:** Lets you validate code paths, transforms, loss functions, and the ensemble logic.\n",
    "\n",
    "> The notebook selects the **first available** dataset in that order and reports which one you’re using.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bbc6433-60bb-41f5-bf3d-44d6e3861490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[data] Loaded OCTMNIST.\n",
      "Dataset: OCTMNIST | classes: 4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def load_datasets() -> Tuple[torch.utils.data.Dataset, torch.utils.data.Dataset, Dict]:\n",
    "    info = {\"name\": None, \"num_classes\": None, \"as_rgb\": False, \"img_size\": 28, \"labels\": None}\n",
    "\n",
    "    # 1) Try OCTMNIST\n",
    "    try:\n",
    "        # placeholder transforms (will be replaced by safe versions in the next cell)\n",
    "        transform_train = T.ToTensor()\n",
    "        transform_eval  = T.ToTensor()\n",
    "\n",
    "        train_ds = OCTMNIST(split=\"train\", transform=transform_train, download=True, as_rgb=False)\n",
    "        val_ds   = OCTMNIST(split=\"val\",   transform=transform_eval,   download=True, as_rgb=False)\n",
    "\n",
    "        info[\"name\"] = \"OCTMNIST\"\n",
    "        info[\"num_classes\"] = len(medmnist.INFO[\"octmnist\"][\"label\"])\n",
    "        info[\"labels\"] = medmnist.INFO[\"octmnist\"][\"label\"]\n",
    "        print(\"[data] Loaded OCTMNIST.\")\n",
    "        return train_ds, val_ds, info\n",
    "    except Exception as e:\n",
    "        print(\"[data] OCTMNIST unavailable, reason:\", e)\n",
    "\n",
    "    # 2) Fallback to FashionMNIST (grayscale 28x28, 10 classes)\n",
    "    try:\n",
    "        transform_train = T.ToTensor()\n",
    "        transform_eval  = T.ToTensor()\n",
    "        train_ds = FashionMNIST(root=\"./data\", train=True,  download=True, transform=transform_train)\n",
    "        val_ds   = FashionMNIST(root=\"./data\", train=False, download=True, transform=transform_eval)\n",
    "        info[\"name\"] = \"FashionMNIST\"\n",
    "        info[\"num_classes\"] = 10\n",
    "        info[\"labels\"] = {i: str(i) for i in range(10)}\n",
    "        print(\"[data] Loaded FashionMNIST.\")\n",
    "        return train_ds, val_ds, info\n",
    "    except Exception as e:\n",
    "        print(\"[data] FashionMNIST unavailable, reason:\", e)\n",
    "\n",
    "    # 3) Final fallback: synthetic blobs (4 classes of noisy blobs, 28x28)\n",
    "    print(\"[data] Using synthetic dataset fallback.\")\n",
    "    n_train, n_val, n_classes = 1200, 300, 4\n",
    "    H, W = 28, 28\n",
    "    def make_blobs(n, classes):\n",
    "        xs, ys = [], []\n",
    "        for c in range(classes):\n",
    "            for _ in range(n//classes):\n",
    "                img = torch.zeros(1, H, W)\n",
    "                rr, cc = torch.meshgrid(torch.arange(H), torch.arange(W), indexing=\"ij\")\n",
    "                cx, cy = 7 + c*5, 7 + (c%2)*10\n",
    "                rad = 5 + (c%3)\n",
    "                mask = ((rr - cx)**2 + (cc - cy)**2) < rad*rad\n",
    "                img[0][mask] = 1.0\n",
    "                img += 0.15*torch.randn_like(img)\n",
    "                img.clamp_(0,1)\n",
    "                xs.append(img)\n",
    "                ys.append(c)\n",
    "        return torch.stack(xs), torch.tensor(ys)\n",
    "    Xtr, ytr = make_blobs(n_train, n_classes)\n",
    "    Xva, yva = make_blobs(n_val, n_classes)\n",
    "    train_ds = TensorDataset(Xtr, ytr)\n",
    "    val_ds   = TensorDataset(Xva, yva)\n",
    "    info[\"name\"] = \"SyntheticBlobs28x28\"\n",
    "    info[\"num_classes\"] = n_classes\n",
    "    info[\"labels\"] = {i: f\"class_{i}\" for i in range(n_classes)}\n",
    "    return train_ds, val_ds, info\n",
    "\n",
    "train_ds, val_ds, DATAINFO = load_datasets()\n",
    "NUM_CLASSES = DATAINFO[\"num_classes\"]\n",
    "print(\"Dataset:\", DATAINFO[\"name\"], \"| classes:\", NUM_CLASSES)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a1ab57-1dfc-455d-88df-00a501ded2b9",
   "metadata": {},
   "source": [
    "## 🤝 Ensemble Strategy\n",
    "\n",
    "We compute each model’s softmax, then **weight** it by its **per-sample confidence** (max softmax value):\n",
    "\n",
    "\\[\n",
    "\\text{blend} = \\sum_{m} w_m \\cdot \\text{softmax}_m, \\quad\n",
    "w_m = \\frac{\\max(\\text{softmax}_m)}{\\sum_j \\max(\\text{softmax}_j)}.\n",
    "\\]\n",
    "\n",
    "This gives more influence to models that are **more certain** about the current sample, improving stability without complex meta-learners.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bae83227-3bad-4766-a98a-bd6a6382b7b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforms set (Windows-safe).\n"
     ]
    }
   ],
   "source": [
    "# Replace any lambda transforms with top-level classes (Windows pickling safe)\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "\n",
    "class AddGaussianNoise(object):\n",
    "    def __init__(self, std=0.05): self.std = std\n",
    "    def __call__(self, x): return (x + self.std * torch.randn_like(x)).clamp(0, 1)\n",
    "\n",
    "transform_train_safe = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.RandomApply([T.GaussianBlur(3)], p=0.3),\n",
    "    T.RandomApply([AddGaussianNoise(std=0.05)], p=0.4),\n",
    "    T.Normalize([.5],[.5]),\n",
    "])\n",
    "transform_eval_safe = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([.5],[.5]),\n",
    "])\n",
    "\n",
    "# Apply to datasets that support .transform (TensorDataset doesn’t use it, which is fine)\n",
    "if hasattr(train_ds, \"transform\"): train_ds.transform = transform_train_safe\n",
    "if hasattr(val_ds, \"transform\"):   val_ds.transform   = transform_eval_safe\n",
    "\n",
    "print(\"Transforms set (Windows-safe).\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28c44b4b-1469-4f1e-8b5b-ed09025b39ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaders ready | num_workers = 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2000, 500)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_loaders_safe(train_ds, val_ds, batch_size=64, val_batch=128):\n",
    "    # Optional subsetting for speed\n",
    "    if isinstance(TRAIN_SUBSET, int):\n",
    "        train_ds = Subset(train_ds, list(range(min(TRAIN_SUBSET, len(train_ds)))))\n",
    "    if isinstance(VAL_SUBSET, int):\n",
    "        val_ds   = Subset(val_ds,   list(range(min(VAL_SUBSET, len(val_ds)))))\n",
    "\n",
    "    # Windows can’t pickle local callables easily → avoid worker processes\n",
    "    nw = 0 if os.name == \"nt\" else 2\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,\n",
    "                              num_workers=nw, pin_memory=(DEVICE==\"cuda\"))\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=val_batch,  shuffle=False,\n",
    "                              num_workers=nw, pin_memory=(DEVICE==\"cuda\"))\n",
    "    return train_loader, val_loader\n",
    "\n",
    "train_loader, val_loader = build_loaders_safe(train_ds, val_ds, batch_size=BATCH_SIZE, val_batch=VAL_BATCH)\n",
    "print(\"Loaders ready | num_workers =\", 0 if os.name==\"nt\" else 2)\n",
    "len(train_loader.dataset), len(val_loader.dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f0365d-92f8-4638-87a5-fa3b34e8879d",
   "metadata": {},
   "source": [
    "## 🧱 Architectures\n",
    "\n",
    "- **ResNet18 (1-channel):** Replace the first conv layer to accept grayscale input; keep the rest unchanged.\n",
    "- **DenseNet121 (28×28 patch):** First conv `3×3, stride=1, pad=1`; remove `pool0` to avoid over-downsampling on small images.\n",
    "- **SimpleCNN:** 3 conv blocks with ReLU + pooling, global average pooling, linear classifier. Tiny but dependable.\n",
    "\n",
    "> Why an ensemble? Individual models may fail unpredictably on niche patterns; averaging calibrated probabilities **reduces variance** and often yields more stable accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b28ab7c-d3e1-4340-af6a-e321687e4c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet18, densenet121\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(16, 32, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(), nn.AdaptiveAvgPool2d(1)\n",
    "        )\n",
    "        self.fc = nn.Linear(64, num_classes)\n",
    "    def forward(self, x):\n",
    "        x = self.net(x); x = x.flatten(1); return self.fc(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee27a0de-9064-4f1a-8d31-2ef436f5f881",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_resnet18(num_classes):\n",
    "    m = resnet18(weights=None)\n",
    "    m.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)  # 1-channel\n",
    "    m.fc = nn.Linear(m.fc.in_features, num_classes)\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a281a21-aad5-49d5-a43b-60f766b682e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_densenet121(num_classes):\n",
    "    m = densenet121(weights=None)\n",
    "    m.features.conv0 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)  # 1-channel\n",
    "    m.classifier = nn.Linear(m.classifier.in_features, num_classes)\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4dd2c03f-b22f-4594-b2a1-eabe95edbeac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models: ResNet DenseNet SimpleCNN\n"
     ]
    }
   ],
   "source": [
    "model_a = make_resnet18(NUM_CLASSES).to(DEVICE)\n",
    "model_b = make_densenet121(NUM_CLASSES).to(DEVICE)\n",
    "model_c = SimpleCNN(NUM_CLASSES).to(DEVICE)\n",
    "\n",
    "print(\"Models:\", type(model_a).__name__, type(model_b).__name__, type(model_c).__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34677b03-e70a-4631-97b6-8512aa61b299",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _fix_inputs(x):\n",
    "    \"\"\"Accept (N,H,W) -> (N,1,H,W) and tuples/lists from some datasets.\"\"\"\n",
    "    if isinstance(x, (list, tuple)):\n",
    "        x = x[0]\n",
    "    if x.ndim == 3:\n",
    "        x = x.unsqueeze(1)   # (N,H,W) -> (N,1,H,W)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7998e9a8-af04-4382-b242-95e8a6c83244",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _fix_targets(y):\n",
    "    \"\"\"Accept (N,), (N,1), one-hot (N,C), numpy arrays, or lists -> (N,) long.\"\"\"\n",
    "    if not isinstance(y, torch.Tensor):\n",
    "        y = torch.as_tensor(y)\n",
    "    if y.ndim > 1:\n",
    "        # If one-hot (float types & >1 classes), convert to indices\n",
    "        if y.ndim == 2 and y.size(-1) > 1 and y.dtype in (\n",
    "            torch.float32, torch.float64, torch.float16, torch.bfloat16\n",
    "        ):\n",
    "            y = y.argmax(dim=-1)\n",
    "        else:\n",
    "            y = y.squeeze(-1)\n",
    "    return y.long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef7647c2-2d80-49c0-b477-5859df809214",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_densenet121_28x28(num_classes: int):\n",
    "    m = densenet121(weights=None)\n",
    "    # 1-channel input + keep resolution: 3x3, stride=1, padding=1\n",
    "    m.features.conv0 = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    # remove the early 3x3 avg pool that would shrink 28x28 too much\n",
    "    m.features.pool0 = nn.Identity()\n",
    "    # classifier for our number of classes\n",
    "    m.classifier = nn.Linear(m.classifier.in_features, num_classes)\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18730153-6f99-4230-afea-2d2e6165cf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_b = make_densenet121_28x28(NUM_CLASSES).to(DEVICE)\n",
    "opt_b = torch.optim.Adam(model_b.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6735f325-0e65-4954-af9c-cb5c0305a884",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, opt, print_every=50, device=None):\n",
    "    device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.train()\n",
    "    ce = nn.CrossEntropyLoss()\n",
    "    running = 0.0\n",
    "    for i, (x, y) in enumerate(tqdm(loader, leave=False)):\n",
    "        x = _fix_inputs(x).to(device)\n",
    "        y = _fix_targets(y).to(device)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        logits = model(x)\n",
    "        loss = ce(logits, y)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        running += loss.item()\n",
    "        if (i + 1) % print_every == 0:\n",
    "            print(f\"step {i+1}/{len(loader)} | loss={running/(i+1):.4f}\")\n",
    "    return running / max(1, len(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b85f8ca6-b0dd-4be6-be4f-54512f654e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, loader, device=None):\n",
    "    device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.eval()\n",
    "    ce = nn.CrossEntropyLoss()\n",
    "    total, correct, total_loss = 0, 0, 0.0\n",
    "    for x, y in loader:\n",
    "        x = _fix_inputs(x).to(device)\n",
    "        y = _fix_targets(y).to(device)\n",
    "        logits = model(x)\n",
    "        loss = ce(logits, y)\n",
    "        total_loss += loss.item()\n",
    "        pred = logits.argmax(1)\n",
    "        correct += (pred == y).sum().item()\n",
    "        total += y.numel()\n",
    "    return correct / max(1, total), total_loss / max(1, len(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "24559efe-2ef0-4e58-801a-80b56c0e30da",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_ensemble(models, loader, device=None):\n",
    "    device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    for m in models: m.eval()\n",
    "    total, correct = 0, 0\n",
    "    for x, y in loader:\n",
    "        x = _fix_inputs(x).to(device)\n",
    "        y = _fix_targets(y).to(device)\n",
    "        probs = []\n",
    "        for m in models:\n",
    "            logits = m(x)\n",
    "            probs.append(F.softmax(logits, dim=1))\n",
    "        stacked = torch.stack(probs)                          # (M, B, C)\n",
    "        conf = stacked.max(dim=2).values                      # (M, B)\n",
    "        weights = conf / (conf.sum(dim=0, keepdim=True) + 1e-9)\n",
    "        blended = (stacked * weights.unsqueeze(2)).sum(dim=0) # (B, C)\n",
    "        pred = blended.argmax(1)\n",
    "        correct += (pred == y).sum().item()\n",
    "        total += y.numel()\n",
    "    return correct / max(1, total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177a6102-3c03-4627-9e7a-8eec7ad74931",
   "metadata": {},
   "source": [
    "## 📈 Results\n",
    "\n",
    "> *Fill these after training on your machine.*\n",
    "\n",
    "- **Dataset used:** `OCTMNIST | FashionMNIST | SyntheticBlobs`\n",
    "- **ResNet18:** `acc = ___`, `val loss = ___`\n",
    "- **DenseNet121:** `acc = ___`, `val loss = ___`\n",
    "- **SimpleCNN:** `acc = ___`, `val loss = ___`\n",
    "- **Ensemble:** `acc = ___`\n",
    "\n",
    "### Observations\n",
    "- The ensemble typically outperforms the weakest model and is often close to the best, while being **more consistent** across runs.\n",
    "- DenseNet patch is essential for 28×28 inputs; otherwise early pooling collapses the spatial map.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "39d3cecd-09cb-4cb0-996e-1ae3863e02a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 1/1] ResNet18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet18  val acc: 0.524 | loss: 1.238\n",
      "\n",
      "[Epoch 1/1] DenseNet121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DenseNet121 val acc: 0.608 | loss: 1.047\n",
      "\n",
      "[Epoch 1/1] SimpleCNN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleCNN  val acc: 0.468 | loss: 1.188\n",
      "\n",
      "Ensemble accuracy (quick run): 0.636\n"
     ]
    }
   ],
   "source": [
    "opt_a = torch.optim.Adam(model_a.parameters(), lr=1e-3)\n",
    "opt_b = torch.optim.Adam(model_b.parameters(), lr=1e-3)\n",
    "opt_c = torch.optim.Adam(model_c.parameters(), lr=1e-3)\n",
    "\n",
    "for e in range(EPOCHS):\n",
    "    print(f\"\\n[Epoch {e+1}/{EPOCHS}] ResNet18\")\n",
    "    train_one_epoch(model_a, train_loader, opt_a)\n",
    "    acc, val_loss = evaluate(model_a, val_loader)\n",
    "    print(f\"ResNet18  val acc: {acc:.3f} | loss: {val_loss:.3f}\")\n",
    "\n",
    "    print(f\"\\n[Epoch {e+1}/{EPOCHS}] DenseNet121\")\n",
    "    train_one_epoch(model_b, train_loader, opt_b)\n",
    "    acc, val_loss = evaluate(model_b, val_loader)\n",
    "    print(f\"DenseNet121 val acc: {acc:.3f} | loss: {val_loss:.3f}\")\n",
    "\n",
    "    print(f\"\\n[Epoch {e+1}/{EPOCHS}] SimpleCNN\")\n",
    "    train_one_epoch(model_c, train_loader, opt_c)\n",
    "    acc, val_loss = evaluate(model_c, val_loader)\n",
    "    print(f\"SimpleCNN  val acc: {acc:.3f} | loss: {val_loss:.3f}\")\n",
    "\n",
    "ens_acc = evaluate_ensemble([model_a, model_b, model_c], val_loader)\n",
    "print(f\"\\nEnsemble accuracy (quick run): {ens_acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3d34580e-5dde-476e-84ea-d9ca42b3d2fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ./checkpoints_oct\\OCTMNIST_resnet18.pt\n",
      "Saved ./checkpoints_oct\\OCTMNIST_densenet121.pt\n",
      "Saved ./checkpoints_oct\\OCTMNIST_simplecnn.pt\n"
     ]
    }
   ],
   "source": [
    "def save_model(model, name):\n",
    "    path = os.path.join(CHECKPOINT_DIR, f\"{name}.pt\")\n",
    "    torch.save(model.state_dict(), path)\n",
    "    print(\"Saved\", path)\n",
    "\n",
    "save_model(model_a, f\"{DATAINFO['name']}_resnet18\")\n",
    "save_model(model_b, f\"{DATAINFO['name']}_densenet121\")\n",
    "save_model(model_c, f\"{DATAINFO['name']}_simplecnn\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0b118b89-115d-4265-8270-8b7147ef52cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x200 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def show_batch(loader, n=8):\n",
    "    x, y = next(iter(loader))\n",
    "    if isinstance(x, (list, tuple)): x = x[0]\n",
    "    if x.ndim == 3: x = x.unsqueeze(1)\n",
    "    imgs = x.clone()\n",
    "    # de-normalize for display\n",
    "    imgs = imgs * 0.5 + 0.5\n",
    "    cols = min(n, 8); rows = int(math.ceil(n/cols))\n",
    "    plt.figure(figsize=(2*cols, 2*rows))\n",
    "    for i in range(min(n, imgs.shape[0])):\n",
    "        plt.subplot(rows, cols, i+1)\n",
    "        plt.imshow(imgs[i,0].cpu(), cmap=\"gray\")\n",
    "        lbl = int(y[i]) if hasattr(y, \"__len__\") else int(y)\n",
    "        plt.title(f\"y={lbl}\")\n",
    "        plt.axis(\"off\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "show_batch(val_loader, n=8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344f8f06-2aa2-4bc6-bba5-b96be0a699c2",
   "metadata": {},
   "source": [
    "## 🩻 Error Analysis (Checklist)\n",
    "\n",
    "Use misclassified samples to guide improvements:\n",
    "- Confusions across **visually similar** classes\n",
    "- Sensitivity to **noise/contrast**\n",
    "- Failure cases under **low-contrast lesions**\n",
    "- Over-confidence: high softmax but wrong label\n",
    "\n",
    "### Next actions\n",
    "- Add **augmentations** (e.g., `RandomAffine`, `Contrast`, or `RandAugment`)\n",
    "- Improve calibration (temperature scaling, label smoothing)\n",
    "- Try **Cosine LR** or longer training\n",
    "- Consider **k-fold ensembling** or a small **stacking head**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ab86aba0-f20b-4d4b-a424-8ea1d3c7e881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred: 0 | True: 3\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def predict_ensemble(x: torch.Tensor):\n",
    "    \"\"\"x: (H,W) or (1,H,W) or (1,1,H,W) in [0,1].\"\"\"\n",
    "    if x.ndim == 2: x = x.unsqueeze(0).unsqueeze(0)\n",
    "    if x.ndim == 3: x = x.unsqueeze(0)         # (1,1,H,W)\n",
    "    if x.shape[1] != 1: x = x[:, :1, ...]\n",
    "    x = (x.to(DEVICE) - 0.5) / 0.5\n",
    "    models = [model_a, model_b, model_c]\n",
    "    for m in models: m.eval()\n",
    "    probs = []\n",
    "    for m in models:\n",
    "        logits = m(x)\n",
    "        probs.append(F.softmax(logits, dim=1))\n",
    "    stacked = torch.stack(probs)                         # (M, 1, C)\n",
    "    conf = stacked.max(dim=2).values                     # (M, 1)\n",
    "    weights = conf / (conf.sum(dim=0, keepdim=True) + 1e-9)\n",
    "    blended = (stacked * weights.unsqueeze(2)).sum(dim=0)  # (1, C)\n",
    "    pred = blended.argmax(1).item()\n",
    "    return pred, blended.squeeze(0).cpu().numpy()\n",
    "\n",
    "# Demo on a validation sample\n",
    "xb, yb = next(iter(val_loader))\n",
    "x0, y0 = (xb[0] if isinstance(xb, torch.Tensor) else xb[0][0]), (int(yb[0]) if hasattr(yb, \"__len__\") else int(yb))\n",
    "pred, prob = predict_ensemble(x0)\n",
    "print(\"Pred:\", pred, \"| True:\", y0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e651ba1-a8a5-4602-bd91-a15228941198",
   "metadata": {},
   "source": [
    "## ✅ Final Notes\n",
    "\n",
    "This notebook builds a **robust, portable image-classification pipeline** for grayscale medical images. It prefers **OCTMNIST (MedMNIST)** and automatically falls back to **FashionMNIST** or a small **synthetic** dataset so the full training–evaluation loop always runs—on CPU and on Windows.\n",
    "\n",
    "Three models are trained—**ResNet18**, **DenseNet121** (patched for 28×28), and a lightweight **SimpleCNN**—and combined with a **confidence-weighted ensemble**. The ensemble reduces variance and typically delivers more stable performance than any single network.\n",
    "\n",
    "Key engineering choices ensure reliability:\n",
    "- **Windows-safe data pipeline** (no lambda transforms; `num_workers=0`)\n",
    "- **DenseNet patch** (stride=1, no initial pooling) to avoid over-downsampling\n",
    "- **Target normalization** to handle `(N,1)` or one-hot labels\n",
    "- **Small, self-contained cells** that are easy to read, debug, and extend\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02a54ee-a848-46b1-9f52-58ddb4153979",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
